{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor_data = torch.tensor([1,2,3])\n",
    "print(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tensor_data = torch.tensor([1,2,3],dtype=torch.float64)  # requires_grad=False , pin_memory=False  , device=torch.device('cuda:0')\n",
    "print(tensor_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PIN MEMORY  ---- \n",
    " \n",
    " the pin_memory parameter is typically used in the data loading process when working with DataLoader objects. When pin_memory is set to True, it enables the data to be directly transferred to the GPU memory (if available) from the host memory (RAM) during the data loading process. This can potentially speed up the data transfer between CPU and GPU, leading to faster training times.\n",
    "\n",
    "However, when pin_memory is set to True, it also consumes additional CPU memory, as it creates a pinned memory buffer to facilitate the data transfer. This may not be an issue if the system has sufficient RAM to spare.\n",
    "\n",
    "If you set pin_memory to False, it means that the data will not be directly transferred to the GPU memory during data loading. Instead, it will stay in the host memory (RAM) and will be transferred to the GPU on-the-fly during the training process. This could result in slightly slower data transfer times between CPU and GPU, but it reduces the additional memory overhead on the CPU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTOGRAD   --- \n",
    "\n",
    "Instead, autograd is a fundamental feature of PyTorch that is always enabled by default. It is responsible for automatic differentiation, which is a key component of training neural networks through backpropagation.\n",
    "\n",
    "Automatic differentiation is a technique that allows the framework to automatically compute gradients of the loss function with respect to all the learnable parameters in the model. Gradients represent the direction and magnitude of the steepest increase of the loss function with respect to each parameter, which is used to update the parameters during training and minimize the loss.\n",
    "\n",
    "When autograd is set to True, PyTorch keeps track of all the operations performed on tensors (inputs, activations, etc.) that are involved in calculating the loss. This creates a computational graph, which is used to compute the gradients when you call the backward() method on the loss tensor. This process propagates the gradients backward through the network and computes the gradients for each parameter.\n",
    "\n",
    "In summary, autograd=True is the default behavior in PyTorch, and it is crucial for enabling automatic differentiation, which is essential for training deep learning models using gradient-based optimization techniques like stochastic gradient descent (SGD) or Adam.\n",
    "\n",
    "You can think of autograd as the engine that powers the automatic computation of gradients, making it easier for developers to define and train complex neural network architectures without having to derive and implement gradients manually for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,2,3,4)\n",
    "print(a.shape)\n",
    "numel_data = torch.numel(a)  # calculates total number of elements in the data matrix\n",
    "print(numel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
